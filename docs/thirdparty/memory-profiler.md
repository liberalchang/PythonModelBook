---
layout: doc
title: memory_profiler - Pythonå†…å­˜ä½¿ç”¨åˆ†æå·¥å…·
permalink: /docs/thirdparty/memory-profiler/
category: thirdparty
tags: [å†…å­˜åˆ†æ, æ€§èƒ½ä¼˜åŒ–, è°ƒè¯•, ç¬¬ä¸‰æ–¹åº“, ç›‘æ§]
description: memory_profileræ˜¯Pythonå†…å­˜ä½¿ç”¨åˆ†æå·¥å…·ï¼Œå¯ä»¥ç›‘æ§å‡½æ•°å’Œä»£ç è¡Œçš„å†…å­˜æ¶ˆè€—ï¼Œå¸®åŠ©è¯†åˆ«å†…å­˜æ³„æ¼å’Œä¼˜åŒ–å†…å­˜ä½¿ç”¨
author: PythonæŠ€æœ¯æ–‡æ¡£å·¥ç¨‹å¸ˆ
date: 2024-01-15
updated: 2024-01-15
version: 1.0
difficulty: "ä¸­çº§"
---

# memory_profiler - Pythonå†…å­˜ä½¿ç”¨åˆ†æå·¥å…·

## ğŸ“ æ¦‚è¿°

memory_profileræ˜¯ä¸€ä¸ªå¼ºå¤§çš„Pythonå†…å­˜ä½¿ç”¨åˆ†æå·¥å…·ï¼Œèƒ½å¤Ÿç›‘æ§Pythonç¨‹åºçš„å†…å­˜æ¶ˆè€—æƒ…å†µã€‚å®ƒæä¾›é€è¡Œå†…å­˜ä½¿ç”¨ç»Ÿè®¡ã€å†…å­˜ä½¿ç”¨è¶‹åŠ¿å›¾ã€æ—¶é—´åºåˆ—å†…å­˜ç›‘æ§ç­‰åŠŸèƒ½ï¼Œæ˜¯è¯†åˆ«å†…å­˜æ³„æ¼ã€ä¼˜åŒ–å†…å­˜ä½¿ç”¨çš„é‡è¦å·¥å…·ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬æ–‡æ¡£çš„å­¦ä¹ ï¼Œä½ å°†èƒ½å¤Ÿï¼š

- æŒæ¡memory_profilerçš„å®‰è£…å’ŒåŸºæœ¬ä½¿ç”¨æ–¹æ³•
- å­¦ä¼šä½¿ç”¨@profileè£…é¥°å™¨è¿›è¡Œå†…å­˜åˆ†æ
- ç†è§£å†…å­˜ä½¿ç”¨æŠ¥å‘Šçš„å„é¡¹æŒ‡æ ‡å«ä¹‰
- æŒæ¡mprofå‘½ä»¤è¡Œå·¥å…·çš„ä½¿ç”¨
- å­¦ä¼šç»˜åˆ¶å†…å­˜ä½¿ç”¨æ—¶é—´åºåˆ—å›¾
- åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨å†…å­˜ä¼˜åŒ–æŠ€å·§
- ç»“åˆå…¶ä»–åˆ†æå·¥å…·è¿›è¡Œç»¼åˆæ€§èƒ½åˆ†æ

## ğŸ“‹ å‰ç½®çŸ¥è¯†

- PythonåŸºç¡€è¯­æ³•å’Œå‡½æ•°å®šä¹‰
- åŸºæœ¬çš„å†…å­˜ç®¡ç†æ¦‚å¿µ
- äº†è§£Pythonæ¨¡å—å’ŒåŒ…çš„ä½¿ç”¨
- åŸºç¡€çš„æ•°æ®å¯è§†åŒ–æ¦‚å¿µï¼ˆå¯é€‰ï¼‰

## ğŸ” è¯¦ç»†å†…å®¹

### å†…å­˜åˆ†æå·¥å…·æ¦‚è¿°

Pythonä¸­å¸¸ç”¨çš„å†…å­˜åˆ†æå·¥å…·åŒ…æ‹¬ï¼š

- **memory_profiler**: é€è¡Œå†…å­˜ä½¿ç”¨åˆ†æ
- **tracemalloc**: Python 3.4+å†…ç½®çš„å†…å­˜è¿½è¸ªå·¥å…·
- **pympler**: è¯¦ç»†çš„å†…å­˜åˆ†æå’Œè°ƒè¯•å·¥å…·  
- **objgraph**: å¯¹è±¡å¼•ç”¨å›¾åˆ†æå·¥å…·
- **guppy/heapy**: å †å†…å­˜åˆ†æå·¥å…·

memory_profilerçš„ä¼˜åŠ¿åœ¨äºç®€å•æ˜“ç”¨ï¼Œèƒ½å¤Ÿç›´è§‚æ˜¾ç¤ºæ¯è¡Œä»£ç çš„å†…å­˜æ¶ˆè€—å˜åŒ–ã€‚

### å®‰è£…æ–¹æ³•

```bash
# åŸºç¡€å®‰è£…
pip install memory-profiler

# å¦‚æœéœ€è¦ç»˜å›¾åŠŸèƒ½ï¼Œå®‰è£…matplotlib
pip install memory-profiler[plotting]
pip install matplotlib

# æˆ–è€…åˆ†åˆ«å®‰è£…
pip install memory-profiler psutil matplotlib
```

## ğŸ’¡ å®é™…åº”ç”¨

### åŸºç¡€ä½¿ç”¨æ–¹æ³•

#### æ–¹æ³•ä¸€ï¼šä½¿ç”¨@profileè£…é¥°å™¨

è¿™æ˜¯æœ€å¸¸ç”¨çš„æ–¹å¼ï¼Œéœ€è¦åœ¨ç›®æ ‡å‡½æ•°ä¸Šæ·»åŠ `@profile`è£…é¥°å™¨ï¼š

```python
# example.py
@profile
def my_func():
    # åˆ›å»ºä¸€ä¸ªå¤§åˆ—è¡¨
    a = [1] * (1000 * 1000)
    
    # åˆ›å»ºå¦ä¸€ä¸ªå¤§åˆ—è¡¨
    b = [2] * (2000 * 1000)
    
    # åˆ é™¤ç¬¬ä¸€ä¸ªåˆ—è¡¨
    del a
    
    return b

if __name__ == '__main__':
    result = my_func()
```

è¿è¡Œåˆ†æï¼š

```bash
# ä½¿ç”¨mprof runæ‰§è¡Œè„šæœ¬
mprof run example.py

# æˆ–è€…ä½¿ç”¨python -m memory_profiler
python -m memory_profiler example.py
```

**è¾“å‡ºç»“æœè§£æï¼š**

```
Filename: example.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
     2     45.3 MiB     45.3 MiB           1   @profile
     3                                         def my_func():
     4     52.1 MiB      6.8 MiB           1       a = [1] * (1000 * 1000)
     5     67.4 MiB     15.3 MiB           1       b = [2] * (2000 * 1000)
     6     59.8 MiB     -7.6 MiB           1       del a
     7     59.8 MiB      0.0 MiB           1       return b
```

**å­—æ®µè¯´æ˜ï¼š**
- **Line #**: è¡Œå·
- **Mem usage**: è¯¥è¡Œæ‰§è¡Œåçš„æ€»å†…å­˜ä½¿ç”¨é‡
- **Increment**: è¯¥è¡Œæ‰§è¡Œå¯¼è‡´çš„å†…å­˜å˜åŒ–ï¼ˆæ­£å€¼è¡¨ç¤ºå¢åŠ ï¼‰
- **Occurrences**: è¯¥è¡Œè¢«æ‰§è¡Œçš„æ¬¡æ•°
- **Line Contents**: ä»£ç å†…å®¹

#### æ–¹æ³•äºŒï¼šç¨‹åºåŒ–ä½¿ç”¨

```python
from memory_profiler import profile

# ç›´æ¥è£…é¥°å‡½æ•°
@profile
def memory_intensive_function():
    # æ¨¡æ‹Ÿå†…å­˜å¯†é›†å‹æ“ä½œ
    data = []
    for i in range(100000):
        data.append([0] * 100)
    
    # å¤„ç†æ•°æ®
    processed = [sum(row) for row in data]
    
    # æ¸…ç†éƒ¨åˆ†æ•°æ®
    del data
    
    return processed

# è¿è¡Œå‡½æ•°
if __name__ == '__main__':
    result = memory_intensive_function()
```

#### æ–¹æ³•ä¸‰ï¼šæ‰‹åŠ¨ç›‘æ§ç‰¹å®šä»£ç å—

```python
from memory_profiler import memory_usage
import time

def my_function():
    # æ¨¡æ‹Ÿä¸€äº›è®¡ç®—
    data = [i**2 for i in range(100000)]
    time.sleep(0.1)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
    return sum(data)

# ç›‘æ§å‡½æ•°æ‰§è¡Œè¿‡ç¨‹ä¸­çš„å†…å­˜ä½¿ç”¨
mem_usage = memory_usage((my_function, ()))
print(f"å†…å­˜ä½¿ç”¨æƒ…å†µ: {mem_usage}")
print(f"æœ€å¤§å†…å­˜ä½¿ç”¨: {max(mem_usage):.2f} MiB")
print(f"æœ€å°å†…å­˜ä½¿ç”¨: {min(mem_usage):.2f} MiB")
```

### é«˜çº§åŠŸèƒ½

#### æ—¶é—´åºåˆ—å†…å­˜ç›‘æ§

```python
from memory_profiler import memory_usage
import time
import numpy as np

def memory_heavy_task():
    """å†…å­˜å¯†é›†å‹ä»»åŠ¡"""
    # é˜¶æ®µ1ï¼šåˆ›å»ºå¤§æ•°ç»„
    arr1 = np.random.random((1000, 1000))
    time.sleep(0.5)
    
    # é˜¶æ®µ2ï¼šåˆ›å»ºæ›´å¤§æ•°ç»„
    arr2 = np.random.random((2000, 2000))
    time.sleep(0.5)
    
    # é˜¶æ®µ3ï¼šæ¸…ç†ç¬¬ä¸€ä¸ªæ•°ç»„
    del arr1
    time.sleep(0.5)
    
    # é˜¶æ®µ4ï¼šè®¡ç®—ç»“æœ
    result = np.sum(arr2)
    time.sleep(0.5)
    
    return result

# ç›‘æ§å†…å­˜ä½¿ç”¨ï¼Œæ¯0.1ç§’é‡‡æ ·ä¸€æ¬¡
mem_usage = memory_usage(
    (memory_heavy_task, ()),
    interval=0.1,
    timeout=3
)

print("æ—¶é—´åºåˆ—å†…å­˜ä½¿ç”¨:")
for i, usage in enumerate(mem_usage):
    print(f"æ—¶é—´ç‚¹ {i*0.1:.1f}s: {usage:.2f} MiB")
```

#### å¤šè¿›ç¨‹å†…å­˜ç›‘æ§

```python
from memory_profiler import memory_usage
import multiprocessing as mp
import time

def worker_function(data_size):
    """å·¥ä½œè¿›ç¨‹å‡½æ•°"""
    # åˆ›å»ºæŒ‡å®šå¤§å°çš„æ•°æ®
    data = [0] * data_size
    time.sleep(1)
    return sum(data)

def monitor_multiprocess():
    """ç›‘æ§å¤šè¿›ç¨‹å†…å­˜ä½¿ç”¨"""
    # åˆ›å»ºè¿›ç¨‹æ± 
    with mp.Pool(processes=4) as pool:
        # æäº¤ä»»åŠ¡
        results = []
        for i in range(4):
            result = pool.apply_async(worker_function, (100000 * (i+1),))
            results.append(result)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for result in results:
            result.get()

# ç›‘æ§å¤šè¿›ç¨‹ç¨‹åºçš„å†…å­˜ä½¿ç”¨
if __name__ == '__main__':
    mem_usage = memory_usage(
        (monitor_multiprocess, ()),
        interval=0.2,
        multiprocess=True
    )
    
    print(f"å¤šè¿›ç¨‹æœ€å¤§å†…å­˜ä½¿ç”¨: {max(mem_usage):.2f} MiB")
```

### å‘½ä»¤è¡Œå·¥å…·ä½¿ç”¨

#### mprofå‘½ä»¤è¯¦è§£

```bash
# 1. è¿è¡Œç¨‹åºå¹¶è®°å½•å†…å­˜ä½¿ç”¨
mprof run --python python example.py

# 2. è¿è¡Œç¨‹åºå¹¶æŒ‡å®šè¾“å‡ºæ–‡ä»¶
mprof run --output memory_profile.dat example.py

# 3. ç»˜åˆ¶å†…å­˜ä½¿ç”¨å›¾
mprof plot

# 4. ç»˜åˆ¶æŒ‡å®šçš„profileæ–‡ä»¶
mprof plot memory_profile.dat

# 5. æ¸…ç†æ‰€æœ‰profileæ–‡ä»¶
mprof clean

# 6. åˆ—å‡ºæ‰€æœ‰profileæ–‡ä»¶
mprof list
```

#### ç”Ÿæˆå†…å­˜ä½¿ç”¨å›¾è¡¨

```python
# memory_plot_example.py
import numpy as np
import matplotlib.pyplot as plt
from memory_profiler import memory_usage
import time

@profile
def create_large_arrays():
    """åˆ›å»ºå¤§å‹æ•°ç»„çš„å‡½æ•°"""
    print("å¼€å§‹åˆ›å»ºæ•°ç»„...")
    
    # é˜¶æ®µ1ï¼šåˆ›å»ºç¬¬ä¸€ä¸ªæ•°ç»„
    arr1 = np.zeros((1000, 1000))
    time.sleep(0.5)
    print("åˆ›å»ºäº†ç¬¬ä¸€ä¸ªæ•°ç»„")
    
    # é˜¶æ®µ2ï¼šåˆ›å»ºç¬¬äºŒä¸ªæ•°ç»„
    arr2 = np.ones((1500, 1500))
    time.sleep(0.5)
    print("åˆ›å»ºäº†ç¬¬äºŒä¸ªæ•°ç»„")
    
    # é˜¶æ®µ3ï¼šè¿›è¡Œè®¡ç®—
    result = np.dot(arr1[:500, :500], arr2[:500, :500])
    time.sleep(0.5)
    print("å®ŒæˆçŸ©é˜µè¿ç®—")
    
    # é˜¶æ®µ4ï¼šæ¸…ç†å†…å­˜
    del arr1, arr2
    time.sleep(0.5)
    print("æ¸…ç†å®Œæˆ")
    
    return result

if __name__ == '__main__':
    result = create_large_arrays()
```

è¿è¡Œå¹¶ç”Ÿæˆå›¾è¡¨ï¼š

```bash
# è¿è¡Œå†…å­˜åˆ†æ
mprof run memory_plot_example.py

# ç”Ÿæˆå†…å­˜ä½¿ç”¨å›¾è¡¨
mprof plot --output memory_usage.png
```

### å®é™…åº”ç”¨æ¡ˆä¾‹

#### æ¡ˆä¾‹1ï¼šæ•°æ®å¤„ç†å†…å­˜ä¼˜åŒ–

```python
import pandas as pd
from memory_profiler import profile
import numpy as np

@profile
def inefficient_data_processing():
    """å†…å­˜æ•ˆç‡è¾ƒä½çš„æ•°æ®å¤„ç†"""
    # åˆ›å»ºå¤§å‹DataFrame
    df = pd.DataFrame({
        'A': np.random.randn(1000000),
        'B': np.random.randn(1000000),
        'C': np.random.randn(1000000)
    })
    
    # åˆ›å»ºå¤šä¸ªå‰¯æœ¬ï¼ˆå†…å­˜æµªè´¹ï¼‰
    df_copy1 = df.copy()
    df_copy2 = df.copy()
    df_copy3 = df.copy()
    
    # è¿›è¡Œè®¡ç®—
    result = df_copy1['A'] + df_copy2['B'] + df_copy3['C']
    
    return result

@profile  
def efficient_data_processing():
    """å†…å­˜æ•ˆç‡è¾ƒé«˜çš„æ•°æ®å¤„ç†"""
    # åˆ›å»ºå¤§å‹DataFrame
    df = pd.DataFrame({
        'A': np.random.randn(1000000),
        'B': np.random.randn(1000000), 
        'C': np.random.randn(1000000)
    })
    
    # ç›´æ¥åœ¨åŸDataFrameä¸Šæ“ä½œï¼Œé¿å…åˆ›å»ºå‰¯æœ¬
    result = df['A'] + df['B'] + df['C']
    
    return result

if __name__ == '__main__':
    print("=== ä½æ•ˆç‡å¤„ç† ===")
    result1 = inefficient_data_processing()
    
    print("\n=== é«˜æ•ˆç‡å¤„ç† ===")
    result2 = efficient_data_processing()
```

#### æ¡ˆä¾‹2ï¼šæœºå™¨å­¦ä¹ æ¨¡å‹å†…å­˜ç›‘æ§

```python
from memory_profiler import profile
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification

@profile
def train_model_with_monitoring():
    """ç›‘æ§æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒçš„å†…å­˜ä½¿ç”¨"""
    print("ç”Ÿæˆè®­ç»ƒæ•°æ®...")
    X, y = make_classification(
        n_samples=100000,
        n_features=20,
        n_informative=15,
        n_redundant=5,
        random_state=42
    )
    
    print("åˆ›å»ºæ¨¡å‹...")
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42
    )
    
    print("å¼€å§‹è®­ç»ƒ...")
    model.fit(X, y)
    
    print("è¿›è¡Œé¢„æµ‹...")
    predictions = model.predict(X)
    
    print("è®¡ç®—å‡†ç¡®ç‡...")
    accuracy = np.mean(predictions == y)
    
    print(f"æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.4f}")
    
    return model, accuracy

if __name__ == '__main__':
    model, acc = train_model_with_monitoring()
```

### ä¸å…¶ä»–å·¥å…·ç»“åˆ

#### ç»“åˆline_profilerè¿›è¡Œç»¼åˆåˆ†æ

```python
# comprehensive_analysis.py
from memory_profiler import profile as memory_profile
from line_profiler import LineProfiler
import numpy as np

@memory_profile
def analyze_both_time_and_memory():
    """åŒæ—¶åˆ†ææ—¶é—´å’Œå†…å­˜çš„å‡½æ•°"""
    # åˆ›å»ºå¤§æ•°ç»„ï¼ˆå†…å­˜æ¶ˆè€—å¤§ï¼‰
    data = np.random.random((5000, 5000))
    
    # è€—æ—¶è®¡ç®—ï¼ˆæ—¶é—´æ¶ˆè€—å¤§ï¼‰
    result1 = np.linalg.svd(data)
    
    # æ›´å¤šå†…å­˜åˆ†é…
    processed = data * 2 + 1
    
    # å†æ¬¡è€—æ—¶è®¡ç®—
    result2 = np.fft.fft2(processed)
    
    return result2

# ä½¿ç”¨line_profilerè¿›è¡Œæ—¶é—´åˆ†æ
def time_analysis():
    lp = LineProfiler()
    lp_wrapper = lp(analyze_both_time_and_memory)
    lp_wrapper()
    lp.print_stats()

if __name__ == '__main__':
    # å†…å­˜åˆ†æä¼šè‡ªåŠ¨æ‰§è¡Œï¼ˆå› ä¸ºæœ‰@memory_profileè£…é¥°å™¨ï¼‰
    print("=== å†…å­˜åˆ†æ ===")
    result = analyze_both_time_and_memory()
    
    print("\n=== æ—¶é—´åˆ†æ ===")
    time_analysis()
```

### æœ€ä½³å®è·µ

#### 1. å†…å­˜æ³„æ¼æ£€æµ‹

```python
from memory_profiler import profile
import gc

@profile
def potential_memory_leak():
    """æ£€æµ‹æ½œåœ¨çš„å†…å­˜æ³„æ¼"""
    data_storage = []
    
    for i in range(1000):
        # åˆ›å»ºå¤§é‡å¯¹è±¡
        large_data = [0] * 10000
        data_storage.append(large_data)
        
        # æ¨¡æ‹ŸæŸäº›æƒ…å†µä¸‹çš„æ¸…ç†
        if i % 100 == 0:
            # éƒ¨åˆ†æ¸…ç†ï¼Œä½†å¯èƒ½ä¸å®Œå…¨
            data_storage = data_storage[-50:]
            gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
    
    return len(data_storage)

if __name__ == '__main__':
    result = potential_memory_leak()
    print(f"æœ€ç»ˆå­˜å‚¨çš„æ•°æ®é‡: {result}")
```

#### 2. å†…å­˜ä½¿ç”¨ä¼˜åŒ–æ¨¡å¼

```python
from memory_profiler import profile
import numpy as np

@profile
def memory_optimization_patterns():
    """å†…å­˜ä¼˜åŒ–çš„å‡ ç§æ¨¡å¼"""
    
    # 1. ä½¿ç”¨ç”Ÿæˆå™¨è€Œä¸æ˜¯åˆ—è¡¨
    def data_generator(n):
        for i in range(n):
            yield i ** 2
    
    # ç”Ÿæˆå™¨ä½¿ç”¨
    gen_sum = sum(data_generator(100000))
    
    # 2. åŠæ—¶åˆ é™¤ä¸éœ€è¦çš„å˜é‡
    large_array = np.random.random((1000, 1000))
    processed = np.sum(large_array, axis=1)
    del large_array  # åŠæ—¶é‡Šæ”¾å†…å­˜
    
    # 3. ä½¿ç”¨numpyçš„å†…å­˜è§†å›¾
    base_array = np.random.random((2000, 2000))
    view_array = base_array[::2, ::2]  # åˆ›å»ºè§†å›¾è€Œä¸æ˜¯å‰¯æœ¬
    
    # 4. åˆ†å—å¤„ç†å¤§æ•°æ®
    def process_large_data_in_chunks(data, chunk_size=1000):
        results = []
        for i in range(0, len(data), chunk_size):
            chunk = data[i:i+chunk_size]
            result = np.sum(chunk)  # å¤„ç†å—
            results.append(result)
            del chunk  # é‡Šæ”¾å—å†…å­˜
        return results
    
    large_data = np.random.random(10000)
    chunk_results = process_large_data_in_chunks(large_data)
    
    return chunk_results

if __name__ == '__main__':
    results = memory_optimization_patterns()
```

## ğŸ”— ç›¸å…³å†…å®¹

- [line_profiler - é€è¡Œæ€§èƒ½åˆ†æå·¥å…·](/docs/thirdparty/line-profiler/)
- [cProfile - æ ‡å‡†åº“æ€§èƒ½åˆ†æå·¥å…·](/docs/stdlib/profile/)
- [Pythonæ ‡å‡†åº“æ–‡æ¡£](/docs/stdlib/)
- [ç¬¬ä¸‰æ–¹åº“å¯¹æ¯”](/docs/thirdparty/)
- [è°ƒè¯•å’Œå¼€å‘å·¥å…·](/docs/thirdparty/#è°ƒè¯•å’Œå¼€å‘å·¥å…·)

## ğŸ“š å‚è€ƒèµ„æº

- [memory_profilerå®˜æ–¹æ–‡æ¡£](https://pypi.org/project/memory-profiler/)
- [Pythonå†…å­˜ç®¡ç†æŒ‡å—](https://docs.python.org/3/c-api/memory.html)
- [NumPyå†…å­˜ä¼˜åŒ–æŠ€å·§](https://numpy.org/doc/stable/user/basics.performance.html)
- [Pandaså†…å­˜ä¼˜åŒ–ç­–ç•¥](https://pandas.pydata.org/docs/user_guide/scale.html)