---
layout: doc
title: Joblib - è½»é‡çº§æµæ°´çº¿å’Œå¹¶è¡Œè®¡ç®—åº“
permalink: /docs/thirdparty/joblib/
category: thirdparty
tags: [joblib, å¹¶è¡Œè®¡ç®—, ç¼“å­˜, æ€§èƒ½ä¼˜åŒ–, æœºå™¨å­¦ä¹ , æ•°æ®ç§‘å­¦]
description: Joblib æ˜¯ä¸“é—¨ç”¨äº Python ä¸­çš„è½»é‡çº§æµæ°´çº¿å’Œå¹¶è¡Œè®¡ç®—çš„åº“ï¼Œæä¾›é«˜æ•ˆçš„ç£ç›˜ç¼“å­˜å’Œå»¶è¿ŸåŠ è½½åŠŸèƒ½ï¼Œç‰¹åˆ«é€‚åˆæœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦é¢†åŸŸ
author: Python æŠ€æœ¯æ–‡æ¡£å·¥ç¨‹å¸ˆ
date: 2024-01-15
updated: 2024-01-15
version: 1.0
difficulty: "ä¸­çº§"
---

# Joblib - è½»é‡çº§æµæ°´çº¿å’Œå¹¶è¡Œè®¡ç®—åº“

## ğŸ“ æ¦‚è¿°

`joblib` æ˜¯ä¸“é—¨ç”¨äº Python ä¸­çš„**è½»é‡çº§æµæ°´çº¿å’Œå¹¶è¡Œè®¡ç®—çš„åº“**ã€‚å®ƒéå¸¸é€‚åˆäºé‚£äº›éœ€è¦**è¿›è¡Œé‡å¤è®¡ç®—æˆ–å¤§è§„æ¨¡æ•°æ®å¤„ç†çš„ä»»åŠ¡**ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ é¢†åŸŸä¸­ã€‚

`joblib` çš„ä¸»è¦ç‰¹ç‚¹æ˜¯**å…¶èƒ½å¤Ÿæä¾›é«˜æ•ˆçš„ç£ç›˜ç¼“å­˜å’Œå»¶è¿ŸåŠ è½½**ï¼Œè¿™æ„å‘³ç€å®ƒå¯ä»¥å°†å‡½æ•°çš„è¿”å›å€¼ç¼“å­˜åˆ°ç£ç›˜ä¸Šï¼Œå½“å†æ¬¡è°ƒç”¨è¯¥å‡½æ•°æ—¶ï¼Œå¦‚æœè¾“å…¥å‚æ•°æ²¡æœ‰æ”¹å˜ï¼Œ`joblib` å°†ç›´æ¥ä»ç¼“å­˜ä¸­åŠ è½½ç»“æœè€Œä¸æ˜¯é‡æ–°è®¡ç®—ã€‚è¿™å¯¹äºé‚£äº›è®¡ç®—æˆæœ¬é«˜æ˜‚çš„å‡½æ•°ç‰¹åˆ«æœ‰ç”¨ã€‚

æ­¤å¤–ï¼Œ`joblib` è¿˜æä¾›äº†**ç®€å•çš„å¹¶è¡Œè®¡ç®—åŠŸèƒ½**ï¼Œä½¿å¾—åœ¨å¤šæ ¸å¿ƒå¤„ç†å™¨ä¸Šè¿è¡Œä»£ç å˜å¾—è½»è€Œæ˜“ä¸¾ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬æ–‡æ¡£çš„å­¦ä¹ ï¼Œä½ å°†èƒ½å¤Ÿï¼š

- ç†è§£ Joblib çš„åŸºæœ¬æ¦‚å¿µå’Œè®¾è®¡ç†å¿µ
- æŒæ¡ Joblib çš„å®‰è£…å’ŒåŸºç¡€é…ç½®æ–¹æ³•
- å­¦ä¼šä½¿ç”¨å†…å­˜ç¼“å­˜åŠŸèƒ½è¿›è¡Œæ€§èƒ½ä¼˜åŒ–
- ç†Ÿç»ƒè¿ç”¨å¹¶è¡Œè®¡ç®—åŠŸèƒ½å¤„ç†å¤§è§„æ¨¡ä»»åŠ¡
- äº†è§£ç£ç›˜æŒä¹…åŒ–å’Œå»¶è¿ŸåŠ è½½æœºåˆ¶
- èƒ½å¤Ÿåœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨ Joblib è¿›è¡Œæ€§èƒ½ä¼˜åŒ–

## ğŸ“‹ å‰ç½®çŸ¥è¯†

- Python åŸºç¡€è¯­æ³•å’Œå‡½æ•°å®šä¹‰
- åŸºæœ¬çš„å¤šè¿›ç¨‹å’Œå¤šçº¿ç¨‹æ¦‚å¿µ
- æ–‡ä»¶ç³»ç»Ÿå’Œç£ç›˜ I/O åŸºç¡€çŸ¥è¯†
- äº†è§£è®¡ç®—å¯†é›†å‹ä»»åŠ¡çš„ç‰¹ç‚¹

## ğŸ” è¯¦ç»†å†…å®¹

### åŸºæœ¬æ¦‚å¿µ

Joblib é‡‡ç”¨å‡½æ•°è£…é¥°å™¨å’Œç®€æ´çš„ API è®¾è®¡ï¼Œä½¿å¾—å¤æ‚çš„ç¼“å­˜å’Œå¹¶è¡Œè®¡ç®—å˜å¾—ç®€å•æ˜“ç”¨ã€‚å¼€å‘è€…åªéœ€è¦å…³æ³¨ä¸šåŠ¡é€»è¾‘ï¼Œè€Œæ— éœ€å¤„ç†åº•å±‚çš„å¤šè¿›ç¨‹ç®¡ç†å’Œç¼“å­˜æœºåˆ¶ã€‚

### å®‰è£…é…ç½®

å®‰è£… `joblib` éå¸¸ç®€å•ï¼Œåªéœ€é€šè¿‡ pip å³å¯å®Œæˆå®‰è£…ã€‚æ‰“å¼€ä½ çš„ç»ˆç«¯æˆ–å‘½ä»¤è¡Œç•Œé¢ï¼Œè¾“å…¥ä»¥ä¸‹å‘½ä»¤ï¼š

```bash
pip install joblib
```

### è¯­æ³•æ ¼å¼

#### å†…å­˜ç¼“å­˜è£…é¥°å™¨
```python
from joblib import Memory

# åˆ›å»ºç¼“å­˜å¯¹è±¡ï¼ˆæ–°ç‰ˆæœ¬ä½¿ç”¨ä½ç½®å‚æ•°æŒ‡å®šç¼“å­˜ç›®å½•ï¼Œæ—§ç‰ˆæœ¬ä¸º cachedir=ï¼‰
memory = Memory('./cache_dir', verbose=0)

@memory.cache
def expensive_function(param1, param2):
    """è¢«ç¼“å­˜çš„å‡½æ•°"""
    return result
```

#### å¹¶è¡Œè®¡ç®—å‡½æ•°
```python
from joblib import Parallel, delayed

# å¹¶è¡Œæ‰§è¡Œå¤šä¸ªä»»åŠ¡
results = Parallel(n_jobs=-1)(delayed(function)(param) for param in params)
```

### å‚æ•°è¯´æ˜

#### Memory ç±»å‚æ•°

| å‚æ•°å | ç±»å‹ | å¿…éœ€ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|------|--------|------|
| location | str | å¦ | None | ç¼“å­˜ç›®å½•è·¯å¾„ï¼ˆæ—§ç‰ˆä¸º cachedir å‚æ•°ï¼‰ |
| verbose | int | å¦ | 0 | æ—¥å¿—è¯¦ç»†ç¨‹åº¦(0-10) |
| compress | bool/int | å¦ | False | æ˜¯å¦å‹ç¼©ç¼“å­˜æ–‡ä»¶ |
| mmap_mode | str | å¦ | None | å†…å­˜æ˜ å°„æ¨¡å¼ |

#### Parallel ç±»å‚æ•°

| å‚æ•°å | ç±»å‹ | å¿…éœ€ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|------|--------|------|
| n_jobs | int | å¦ | 1 | å¹¶è¡Œä½œä¸šæ•°(-1è¡¨ç¤ºæ‰€æœ‰CPU) |
| backend | str | å¦ | 'loky' | å¹¶è¡Œåç«¯('loky'/'threading'/'multiprocessing') |
| verbose | int | å¦ | 0 | è¿›åº¦æ˜¾ç¤ºè¯¦ç»†ç¨‹åº¦ |
| timeout | float | å¦ | None | è¶…æ—¶æ—¶é—´(ç§’) |

### è¿”å›å€¼

| å‡½æ•° | è¿”å›ç±»å‹ | è¯´æ˜ |
|------|----------|------|
| Memory.cache | è£…é¥°å™¨ | è¿”å›ç¼“å­˜è£…é¥°å™¨å‡½æ•° |
| Parallel() | list | è¿”å›å¹¶è¡Œæ‰§è¡Œç»“æœåˆ—è¡¨ |

## ğŸ’¡ å®é™…åº”ç”¨

### åŸºç¡€ç”¨æ³•

#### ç¤ºä¾‹ä¸€ï¼šä½¿ç”¨å†…å­˜ç¼“å­˜

å‡è®¾ä½ æœ‰ä¸€ä¸ªè®¡ç®—æˆæœ¬å¾ˆé«˜çš„å‡½æ•°ï¼Œä½ å¸Œæœ›èƒ½å¤Ÿä¿å­˜å®ƒçš„è®¡ç®—ç»“æœä»¥ä¾¿å¿«é€Ÿé‡ç”¨ï¼š

```python
from joblib import Memory

# å®šä¹‰ç¼“å­˜ç›®å½•
cachedir = './my_cache'
memory = Memory(cachedir, verbose=0)

@memory.cache
def expensive_computation(a, b):
    """è®¡ç®—å¯†é›†å‹å‡½æ•°ç¤ºä¾‹"""
    print("Computing expensive_computation...")
    # æ¨¡æ‹Ÿå¤æ‚è®¡ç®—
    import time
    time.sleep(2)  # æ¨¡æ‹Ÿ2ç§’çš„è®¡ç®—æ—¶é—´
    return a * b + a / b

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œå°†è®¡ç®—å¹¶ç¼“å­˜ç»“æœ
print("ç¬¬ä¸€æ¬¡è°ƒç”¨:")
result = expensive_computation(2, 3)
print(f"ç»“æœ: {result}")

# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼Œå°†ç›´æ¥ä»ç¼“å­˜åŠ è½½ç»“æœ
print("\nç¬¬äºŒæ¬¡è°ƒç”¨:")
result = expensive_computation(2, 3)
print(f"ç»“æœ: {result}")
```

#### ç¤ºä¾‹äºŒï¼šå¹¶è¡Œè®¡ç®—

å¦‚æœä½ æœ‰å¤šä¸ªç‹¬ç«‹çš„ä»»åŠ¡éœ€è¦æ‰§è¡Œï¼Œå¯ä»¥åˆ©ç”¨ `joblib` çš„ `Parallel` å’Œ `delayed` åŠŸèƒ½å¹¶è¡Œå¤„ç†ä»¥èŠ‚çœæ—¶é—´ï¼š

```python
from joblib import Parallel, delayed
import time

def process(number):
    """å¤„ç†å•ä¸ªæ•°å­—çš„å‡½æ•°"""
    # æ¨¡æ‹Ÿä¸€äº›è®¡ç®—
    time.sleep(0.1)
    return number * number

# ä¸²è¡Œå¤„ç†
print("ä¸²è¡Œå¤„ç†:")
start_time = time.time()
results_serial = [process(i) for i in range(10)]
serial_time = time.time() - start_time
print(f"ä¸²è¡Œç»“æœ: {results_serial}")
print(f"ä¸²è¡Œè€—æ—¶: {serial_time:.2f}ç§’")

# å¹¶è¡Œå¤„ç†
print("\nå¹¶è¡Œå¤„ç†:")
start_time = time.time()
results_parallel = Parallel(n_jobs=2)(delayed(process)(i) for i in range(10))
parallel_time = time.time() - start_time
print(f"å¹¶è¡Œç»“æœ: {results_parallel}")
print(f"å¹¶è¡Œè€—æ—¶: {parallel_time:.2f}ç§’")
```

### é«˜çº§ç”¨æ³•

#### å¸¦è¿›åº¦æ˜¾ç¤ºçš„å¹¶è¡Œè®¡ç®—

```python
from joblib import Parallel, delayed
import time

def heavy_computation(n):
    """æ¨¡æ‹Ÿé‡å‹è®¡ç®—ä»»åŠ¡"""
    time.sleep(0.5)  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
    return n ** 2

# ä½¿ç”¨ verbose å‚æ•°æ˜¾ç¤ºè¿›åº¦
results = Parallel(n_jobs=4, verbose=10)(
    delayed(heavy_computation)(i) for i in range(20)
)
print(f"è®¡ç®—ç»“æœ: {results}")
```

#### æŒä¹…åŒ–ç¼“å­˜ä¸å‹ç¼©

```python
from joblib import Memory
import numpy as np

# åˆ›å»ºå¸¦å‹ç¼©çš„ç¼“å­˜
memory = Memory('./compressed_cache', verbose=1, compress=True)

@memory.cache
def generate_large_array(size):
    """ç”Ÿæˆå¤§å‹æ•°ç»„çš„å‡½æ•°"""
    print(f"ç”Ÿæˆå¤§å°ä¸º {size} çš„æ•°ç»„...")
    return np.random.random(size)

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œç”Ÿæˆå¹¶ç¼“å­˜
large_data = generate_large_array(1000000)
print(f"æ•°ç»„å½¢çŠ¶: {large_data.shape}")

# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼Œä»ç¼“å­˜åŠ è½½
large_data_cached = generate_large_array(1000000)
print(f"ä»ç¼“å­˜åŠ è½½çš„æ•°ç»„å½¢çŠ¶: {large_data_cached.shape}")
```

### å®é™…æ¡ˆä¾‹

#### 1. æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒä¼˜åŒ–

```python
from joblib import Memory, Parallel, delayed
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import numpy as np

# åˆ›å»ºç¼“å­˜
memory = Memory('./ml_cache', verbose=1)

@memory.cache
def load_and_preprocess_data():
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    print("åŠ è½½å’Œé¢„å¤„ç†æ•°æ®...")
    # æ¨¡æ‹Ÿæ•°æ®åŠ è½½å’Œé¢„å¤„ç†
    X = np.random.random((1000, 20))
    y = np.random.randint(0, 2, 1000)
    return X, y

def train_model_with_params(n_estimators, max_depth):
    """ä½¿ç”¨ç‰¹å®šå‚æ•°è®­ç»ƒæ¨¡å‹"""
    X, y = load_and_preprocess_data()  # è¿™ä¼šä½¿ç”¨ç¼“å­˜
    model = RandomForestClassifier(
        n_estimators=n_estimators, 
        max_depth=max_depth, 
        random_state=42
    )
    scores = cross_val_score(model, X, y, cv=3)
    return np.mean(scores)

# å¹¶è¡Œç½‘æ ¼æœç´¢
param_combinations = [
    (50, 5), (50, 10), (100, 5), (100, 10), (200, 5), (200, 10)
]

print("å¼€å§‹å¹¶è¡Œæ¨¡å‹è®­ç»ƒ...")
scores = Parallel(n_jobs=-1, verbose=10)(
    delayed(train_model_with_params)(n_est, depth) 
    for n_est, depth in param_combinations
)

for params, score in zip(param_combinations, scores):
    print(f"å‚æ•° {params}: å¾—åˆ† {score:.4f}")
```

#### 2. å¤§è§„æ¨¡æ•°æ®å¤„ç†æµæ°´çº¿

```python
from joblib import Memory, Parallel, delayed
import pandas as pd
import numpy as np

# è®¾ç½®ç¼“å­˜
memory = Memory('./data_pipeline_cache', verbose=1)

@memory.cache
def load_raw_data(file_path):
    """åŠ è½½åŸå§‹æ•°æ®"""
    print(f"åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
    # æ¨¡æ‹ŸåŠ è½½å¤§å‹CSVæ–‡ä»¶
    return pd.DataFrame({
        'id': range(10000),
        'value': np.random.random(10000),
        'category': np.random.choice(['A', 'B', 'C'], 10000)
    })

@memory.cache
def clean_data(data):
    """æ¸…æ´—æ•°æ®"""
    print("æ¸…æ´—æ•°æ®...")
    # æ¨¡æ‹Ÿæ•°æ®æ¸…æ´—æ“ä½œ
    cleaned = data.dropna()
    cleaned['value_normalized'] = (cleaned['value'] - cleaned['value'].mean()) / cleaned['value'].std()
    return cleaned

def process_category(data, category):
    """å¤„ç†ç‰¹å®šç±»åˆ«çš„æ•°æ®"""
    category_data = data[data['category'] == category]
    # æ¨¡æ‹Ÿå¤æ‚å¤„ç†
    result = {
        'category': category,
        'count': len(category_data),
        'mean_value': category_data['value_normalized'].mean(),
        'std_value': category_data['value_normalized'].std()
    }
    return result

# æ•°æ®å¤„ç†æµæ°´çº¿
def data_pipeline():
    # ç¬¬1æ­¥ï¼šåŠ è½½æ•°æ®ï¼ˆä¼šè¢«ç¼“å­˜ï¼‰
    raw_data = load_raw_data('dummy_file.csv')
    
    # ç¬¬2æ­¥ï¼šæ¸…æ´—æ•°æ®ï¼ˆä¼šè¢«ç¼“å­˜ï¼‰
    clean_data_result = clean_data(raw_data)
    
    # ç¬¬3æ­¥ï¼šå¹¶è¡Œå¤„ç†ä¸åŒç±»åˆ«
    categories = ['A', 'B', 'C']
    results = Parallel(n_jobs=-1)(
        delayed(process_category)(clean_data_result, cat) 
        for cat in categories
    )
    
    return results

# è¿è¡Œæµæ°´çº¿
print("è¿è¡Œæ•°æ®å¤„ç†æµæ°´çº¿...")
pipeline_results = data_pipeline()
for result in pipeline_results:
    print(f"ç±»åˆ« {result['category']}: æ•°é‡={result['count']}, å‡å€¼={result['mean_value']:.4f}")
```

#### 3. æ‰¹é‡æ–‡ä»¶å¤„ç†

```python
from joblib import Parallel, delayed, Memory
import os
import time

memory = Memory('./file_processing_cache', verbose=1)

@memory.cache
def process_file(file_path):
    """å¤„ç†å•ä¸ªæ–‡ä»¶"""
    print(f"å¤„ç†æ–‡ä»¶: {file_path}")
    # æ¨¡æ‹Ÿæ–‡ä»¶å¤„ç†ï¼ˆè¯»å–ã€è½¬æ¢ã€ä¿å­˜ï¼‰
    time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
    file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
    return {
        'file': file_path,
        'size': file_size,
        'processed_at': time.time()
    }

def batch_file_processing(file_list, n_jobs=4):
    """æ‰¹é‡å¤„ç†æ–‡ä»¶"""
    print(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(file_list)} ä¸ªæ–‡ä»¶...")
    
    results = Parallel(n_jobs=n_jobs, verbose=5)(
        delayed(process_file)(file_path) for file_path in file_list
    )
    
    return results

# æ¨¡æ‹Ÿæ–‡ä»¶åˆ—è¡¨
file_list = [f"file_{i:03d}.txt" for i in range(50)]

# æ‰§è¡Œæ‰¹é‡å¤„ç†
batch_results = batch_file_processing(file_list)
print(f"å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(batch_results)} ä¸ªæ–‡ä»¶")
```

## âš ï¸ æ³¨æ„äº‹é¡¹

- **ç¼“å­˜ç›®å½•ç®¡ç†**: å®šæœŸæ¸…ç†ç¼“å­˜ç›®å½•ï¼Œé¿å…å ç”¨è¿‡å¤šç£ç›˜ç©ºé—´
- **å†…å­˜ä½¿ç”¨**: å¤§å‹æ•°æ®ç¼“å­˜æ—¶æ³¨æ„å†…å­˜ä½¿ç”¨æƒ…å†µ
- **å¹¶è¡Œåº¦è®¾ç½®**: `n_jobs=-1` ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒï¼Œå¯èƒ½å½±å“ç³»ç»Ÿå“åº”é€Ÿåº¦
- **å‡½æ•°çº¯åº¦**: è¢«ç¼“å­˜çš„å‡½æ•°åº”è¯¥æ˜¯çº¯å‡½æ•°ï¼Œç›¸åŒè¾“å…¥æ€»æ˜¯äº§ç”Ÿç›¸åŒè¾“å‡º
- **åºåˆ—åŒ–é™åˆ¶**: ç¡®ä¿å‡½æ•°å‚æ•°å’Œè¿”å›å€¼å¯ä»¥è¢«pickleåºåˆ—åŒ–
- **çº¿ç¨‹å®‰å…¨**: åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸­ä½¿ç”¨æ—¶æ³¨æ„çº¿ç¨‹å®‰å…¨é—®é¢˜
- **é”™è¯¯å¤„ç†**: å¹¶è¡Œæ‰§è¡Œæ—¶é€‚å½“å¤„ç†å¼‚å¸¸ï¼Œé¿å…å•ä¸ªä»»åŠ¡å¤±è´¥å½±å“æ•´ä½“

## ğŸ”— ç›¸å…³å†…å®¹

- [NumPy æ•°å€¼è®¡ç®—åº“ - å®Œæ•´å­¦ä¹ æŒ‡å—](./numpy/)
- [Pandas æ•°æ®åˆ†æåº“ - å®Œæ•´å­¦ä¹ æŒ‡å—](./pandas/)
- [Typer - ç°ä»£åŒ– Python CLI æ¡†æ¶](./typer/)

## ğŸ“š æ‰©å±•é˜…è¯»

- [Joblib å®˜æ–¹æ–‡æ¡£](https://joblib.readthedocs.io/)
- [Joblib GitHub é¡¹ç›®](https://github.com/joblib/joblib)
- [Python å¹¶è¡Œè®¡ç®—æœ€ä½³å®è·µ](https://docs.python.org/3/library/concurrent.futures.html)
- [æœºå™¨å­¦ä¹ ä¸­çš„å¹¶è¡Œè®¡ç®—ä¼˜åŒ–](https://scikit-learn.org/stable/computing/parallelism.html)
- [é˜…è¯»æ›´å¤š - zglg.work](https://zglg.work)

## ğŸ·ï¸ æ ‡ç­¾

`joblib` `å¹¶è¡Œè®¡ç®—` `ç¼“å­˜` `æ€§èƒ½ä¼˜åŒ–` `æœºå™¨å­¦ä¹ ` `æ•°æ®ç§‘å­¦`

---

**æœ€åæ›´æ–°**: 2024-01-15  
**ä½œè€…**: Python æŠ€æœ¯æ–‡æ¡£å·¥ç¨‹å¸ˆ  
**ç‰ˆæœ¬**: 1.0